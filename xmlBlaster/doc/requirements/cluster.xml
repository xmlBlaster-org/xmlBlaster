<?xml version='1.0' encoding='ISO-8859-1' ?>
<!DOCTYPE requirement SYSTEM "requirement.dtd">

<requirement id='cluster' type='NEW' prio='HIGH' status="INWORK">
   <topic>Multiple xmlBlaster instances can build a cluster following the master/slave paradigm</topic>
   <description>
      <p>
      We define a cluster as a configuration where more than one xmlBlaster
      server instance is running and those instances know of each other.
      The instances may run on the same host or distributed over the internet.
      </p>
      <p>
      All clustering abilities for xmlBlaster reduce to the simple master/slave approach.
      This cluster approach is easy to understand as we are not leaving the MoM paradigm
      to support clustering.
      </p>
      <p>
      An important part in clustering is the <i>discovery and lookup</i>.
      How to find other cluster nodes and access in depth
      node informations from them, and how to keep those informations up to date.
      This is addressed with the publish/subscribe idea as well.
      XmlBlaster nodes store their cluster informations in messages, so other nodes
      can subsrcribe to this data. If necessary one xmlBlaster is running as a 'naming service'
      holding the informations of all available xmlBlaster instances.
      </p>
      <p>
      In the following examples, we use the term <i>xmlBlaster server instance</i> and
      <i>xmlBlaster node</i> or just <i>node</i> interchangeable.
      </p>
      <center><p>
      <img src="cluster.gif" border="0" title="XmlBlaster cluster example" alt="Example for a typical xmlBlaster cluster" />
      </p></center>
      <p>
      In this example we have three xmlBlaster instances running, each of them has
      a unique cluster node ID, here the names <i>heron</i>, <i>golan</i> and <i>avalon</i>.
      </p>
      <p>
      Each of the nodes have an arbitrary number of clients attached. The clients
      can publish or subscribe to any message in the cluster, and may send PtP messages
      to any other client.
      </p>
      
      <hr />

      <h3>
      Clustering in our sense covers the following topics:
      </h3>
      <ol>
      <li><h4>Scalability:</h4><br />
      An xmlBlaster server instance allows to have any number of slave xmlBlaster server instances
      for specific message domains. These slaves can have further slaves again.
      This allows distributing messages to an almost unlimited number of clients.
      Note that one xmlBlaster node can be a master for some messages and a slave for
      other message types simultaneously.
      <p>
      <img src="cluster.scalability.gif" border="0" title="XmlBlaster cluster example" alt="Example for a typical xmlBlaster cluster" />
      </p>
      <p>
      The example shows a tree like configuration of xmlBlaster nodes. In this way
      we can connect an almost unlimited number of clients. Every child leaf supplies
      a certain amount of slaves, which supplies other slaves which finally supply
      clients with message updates. The slaves are caching the messages and respond to
      request of their clients directly. The cache is always up to date as it is real time
      updated according to the publish/subscribe paradigm. 
      With every child level in the tree the latency increases
      for typically 5-40 milliseconds (intranet) for new published message updates.
      Note that the publisher does not need to be connected to the master node, the client in the
      down left edge of the picture is publishing as well.
      </p>
      <p>
      We introduce the term <i>stratum</i> as the distance of a node from the master. This is done
      in analogy to the network time protocol (NTP). A stratum of 0 is the master itself,
      1 is the first slave and stratum=2 would be bilbo in the above picture.
      </p>
      </li>
      <li><h4>Availability (Failover):</h4><br />
      An xmlBlaster slave may adopt the master role for selective message types
      if the current xmlBlaster master fails.
      <p>
      <img src="cluster.availability.gif" border="0" title="XmlBlaster cluster example" alt="Example for a typical xmlBlaster cluster" />
      </p>
      <p>
      Autonomous failure recovery without distinct cluster manager (as single point of contact).
      </p>
      <p>
      We have three different failure situations to cover:
      </p>
      <ul>
      <li>Master failure:<br />
      This is the example in the above picture.
      </li>
      <li>Slave failure:<br />
      frodo dies, bilbo needs to rearrange (see figure 1).
      Bilbo needs to know the current cluster situation.
      </li>
      <li>Client reconnect:<br />
      Client looses connection to its xmlBlaster instance and
      needs to find another one.
      </li>
      </ul>
      </li>
      <li><h4>Logical separation based on message domains:</h4><br />
      One xmlBlaster instance can be the master for selective messages (e.g. for
      stock exchange messages) and be slave for other messages (e.g. for 
      air traffic radar tracks or rugby news) simultaneously, we call that <i>cluster message domains</i>.
      Consequently a client interested in all informations only needs to connect
      to one xmlBlaster server instance.
      <p>
      <img src="cluster.domain.gif" border="0" title="XmlBlaster cluster example" alt="Example for a typical xmlBlaster cluster" />
      </p>
      <p>
      As we can see the node <i>heron</i> is master of messages of the domain "STOCK_EXCHANGE" but
      caches "RUGBY_NEWS" as well.
      </p>
      </li>
      <li><h4>Load balancing:</h4><br />
      An xmlBlaster cluster allows to have more than one master server for
      a specific message domain. The master nodes are <i>mirrored</i> instances
      for those messages.
      Published messages reach all master nodes.
      Subscribed messages are retrieved using a load balance algorithm.
      <p>
      <img src="cluster.loadbalance.gif" border="0" title="XmlBlaster cluster example" alt="Example for a typical xmlBlaster cluster" />
      </p>
      <p>
      In the above scenario heron1 and heron2 share their knowledge.
      Slave nodes can choose which of those servers to use.
      </p>
      </li>
      </ol>

      <hr />


      <h3>
      Implementation overview:
      </h3>
      
      <p>
      We have to code and manage two logical mapping functionalities:
      </p>
      <ol>
         <li><h4>Find out who is the master of a message</h4><br />
         We have a message and need to map it to a master.
         The decision can be based on any information in the message.
         As a default we supply a simple key attribute based approach:
         An XML attribute <i>domain</i> is used with the
         message &lt;key> tag, for example:<br />
         <pre>&lt;key domain='STOCK_EXCHANGE'/></pre>
         The domain based approach maps domain names to cluster node IDs.
         Please see the examples below.<br />
         Note that this simple domain name approach has severe drawbacks:
         The clients need to know beforehand to which domain a message belongs. If the
         domains change, all clients need to be recompiled/restarted/reconfigured.
         A rule based approach (see the XPATH example in the Example section)
         is generic and addresses this issue.
         </li>
         <li><h4>Map a cluster node ID to the physical xmlBlaster instance</h4><br />
         After step 1. we need to find out how to reach the physical xmlBlaster instance.
         We have its cluster node ID and need to get the CORBA IOR, XmlRpc URL,
         RMI registry entry or the socket to be able to communicate.
         </li>
      </ol>
      
      <br />
      <p>
      The cluster specific features are
      </p>
            
      <ul>
      <li>Multi xmlBlaster instances<br />
      If multiple xmlBlaster servers are running, every message is assigned to exactly one
      master server (if no load balancing is switched on).
      The different xmlBlaster servers login to each other like other clients
      and act as slave to messages which they are not master from.
      </li>

      <li>Connections between xmlBlaster instances<br />
      This login is done in 'lazy' mode. As soon as a client requests a message
      for which the local server is not master, this server does a login to the master
      and subscribes the message from there. The message is than cached locally and further
      local requests from clients are handled locally. The cache is always up to date, as
      the slave has subscribed that info from the master.
      An adjustable expire date clears the cached message.
      </li>

      <li>Routing of published messages<br />
      If a message is published from a data source to a slave server, the message is routed directly
      to the master server (which may be a slave as well, forwarding the message to the real master
      with stratum level equals zero).
      If the master server is currently offline, it is queued
      for this 'client' until it logs in.<br />

      </li>

      <li>Plugin interface for master discovery<br />
      XmlBlaster supplies a plugin interface for your own logic to identify the master cluster node id.
      The default implementation uses the
      <pre>&lt;key domain='RUGBY'></pre> attribute (see examples below).
      </li>

      <li>Plugin interface for load balancing logic<br />
      XmlBlaster supplies a plugin interface for the load balancing logic.
      The default load balancing logic uses the round robin approach.
      </li>

      <li>Default master<br />
      The xmlBlaster node is domain master for its directly connected clients
      as a default. If a client is publishing/subscribing etc. without an
      explicitly specified domain, the messages are handled in the local
      xmlBlaster instance (as if no cluster is existing).
      If domain='NO_DEFAULT' is set another node is searched which
      accepts default messages. A node can set domain="GLOBAL_DEFAULT" to
      accept messages with default domain from other nodes.
      </li>

      <li>Master unknown<br />
      If a node receives a message and can't find the master,
      the message is accepted and queued for the master.
      If later a master appears the messages are flushed.
      </li>

      <li>Forward a client<br />
      If an xmlBlaster node is stopped, it has the ability to inform
      its clients and pass them a forward address of another
      xmlBlaster node which is willing to take care of them.
      </li>

      <li>PtP messages<br />
      To allow global delivery we introduce a unique naming schema,
      it is based on the URL naming synstax and allows addressing
      any client at any node.
      The routing setup for such messages is not specified yet.
      Clustering applies for Publish/Subscribe messages only, PtP messages
      are just routed to their destination using the cluster node as hops
      to find the goal.
      </li>

      <li>Multiple masters for same domain<br />
      If multiple nodes acquire the master mode for a certain domain
      there are two approaches.<br />
      Messages which are published are sent to all masters.<br />
      Messages which are accessed with get() or subscribe() are handled
      by the load balancing algorithm. The default implementation is a round robin.
      </li>

      <li>Behavior of the different xmlBlaster methods<br />
      XmlBlaster supports only a small number of methods to invoke.
      In the context of clustering they can be categorized into these groups:
        <ol>
         <li>Methods with local scope:<br />
           These are connect(), disconnect() and ping(). The have only
           a local scope between a client and its direct xmlBlaster node.
         </li>
         <li>Write access:<br />
           These are publish(), publishOneway() and erase(). Such invocations are passed
           to the master node. The new state than cascades the usual way
           to the connected slaves.
         </li>
         <li>Read access:<br />
           These are get() and subscribe() and unSubscribe().
           Such invocations are usually handled by the local xmlBlaster node,
           which itself forwards appropriate requests to the master.
         </li>
         <li>Callbacks:<br />
           These are update(), updateOneway() and ping().
           The behavior in a cluster is not specified yet.
         </li>
        </ol>
      </li>
      </ul>

   </description>


   <example lang="Java" type="HTML">
      Here are xmlBlaster internal messages which support clustering:
      <p>
      <table border="1">
         <tr><td>1</td><td>Mapping of a cluster node ID to a physical xmlBlaster instance</td></tr>
         <tr><td>key</td><td><pre>
   &lt;key oid='__sys__cluster.node:heron'>
      &lt;__sys__cluster.node/>
   &lt;/key>
         </pre></td></tr>
         <tr><td>content</td><td><pre>
   <!-- In future we could distinguish normal clients, slave clients
        and address based maxConnections -->

   &lt;clusternode id='heron' maxConnections='800'>
      &lt;address type='IOR'>IOR:00044550005...&lt;/address>
      &lt;address type='XML-RPC' maxConnections='20'>http://www.mars.universe:8080/RPC2&lt;/address>
      &lt;callback type='XML-RPC'>http://www.mars.universe:8081/RPC2&lt;/callback>
      &lt;backupnode>
         &lt;clusternode id='avalon'/>  &lt;!-- first failover node -->
         &lt;clusternode id='golan'/>   &lt;!-- second backup node -->
      &lt;/backupnode>
      &lt;nameservice>true&lt;/nameservice>
   &lt;/clusternode>
         </pre></td></tr>
      </table>
      </p>

      <p>
      <table border="1">
         <tr><td>2</td><td>Determine the master: Mapping of messages to cluster node IDs</td><td>Comments</td></tr>
         <tr><td>2a) key</td><td><pre>
   &lt;key oid='__sys__cluster.node.domainmapping:heron'>
      &lt;__sys__cluster.node.domainmapping/>
   &lt;/key>
         </pre></td>
         <td> </td>
         </tr>
         <tr><td>content</td><td><pre>
   // This is a master for domainless messages, for football and rugby
   &lt;clusternode id='heron'>
      &lt;master stratum='0'>
         &lt;key domain=''/>           &lt;!-- default setting -->
         &lt;key domain='football'/>
         &lt;key domain='rugby'/>
      &lt;/master>
   &lt;/clusternode>
         </pre></td>
         <td>This cluster node is the master of the domain 'football' and 'rugby'.
         Messages without a domain specified are treated locally as well.</td>
         </tr>

         <tr><td>2b) key</td><td><pre>
   &lt;key oid='__sys__cluster.node.domainmapping:frodo'>
      &lt;__sys__cluster.node.domainmapping/>
   &lt;/key>
         </pre></td>
         <td> </td>
         </tr>
         <tr><td>content</td><td><pre>
   // This is a slave for everything
   &lt;clusternode id='frodo'>
      &lt;master stratum='0'>
         &lt;key domain='NO_DEFAULT'/> &lt;!-- forward empty domains -->
      &lt;/master>
         </pre></td>
         <td>Messages without a domain specified are normally treated
         by its local xmlBlaster node. Here this is switched off</td>
         </tr>

         <tr><td>2c) key</td><td><pre>
   &lt;key oid='__sys__cluster.node.domainmapping:bilbo'>
      &lt;__sys__cluster.node.domainmapping/>
   &lt;/key>
         </pre></td>
         <td> </td>
         </tr>
         <tr><td>content</td><td><pre>
   // Bilbo is master of RECIPIES and local clients,
   // but slave for everything else
   &lt;clusternode id='0'>
      &lt;master stratum='0'>
         &lt;key domain=''/>
         &lt;key domain='RECIPIES'/>
      &lt;/master>

      // refid points to a node one stratum closer to master
      &lt;master stratum='2' refid='frodo' />
      &lt;/master>
   &lt;/clusternode>
         </pre></td>
         <td>Bilbo is slave of a slave for heron messages.
         Therefore it is stratum = 2 (two steps from the master).
         It only knows bilbo, its direct parent node.
         </td>
         </tr>

         <tr><td>2d) key</td><td><pre>
   &lt;key oid='__sys__cluster.node.domainmapping:heron'>
      &lt;__sys__cluster.node.domainmapping/>
   &lt;/key>
         </pre></td>
         <td> </td>
         </tr>
         <tr><td>content</td><td><pre>
   // The master is determined in a generic way (no explicit domain)
   &lt;clusternode id='heron'>
      &lt;master>
         &lt;key oid='radar.track'/>
         &lt;key queryType='XPATH'>
            //STOCK_EXCHANGE
         &lt;/key>
      &lt;/master>
   &lt;/clusternode>
         </pre></td>
         <td>Approach without domains. Every message is filtered
         with the given rules. If one of the rules matches, we are the master
         of this message</td>
         </tr>

         <tr><td>2e) key</td><td><pre>
   &lt;key oid='__sys__cluster.node.domainmapping:heron'>
      &lt;__sys__cluster.node.domainmapping/>
   &lt;/key>
         </pre></td>
         <td> </td>
         </tr>
         <tr><td>content</td><td><pre>
   // The master is determined with a customer plugin
   // (no explicit domain)
   &lt;clusternode id='heron'>
      &lt;master>
         Java plugin (implements I_MapMsgToMasterId)
      &lt;/master>
   &lt;/clusternode>
         </pre></td>
         <td>Approach without domains. Every message is filtered by
         a user supplied plugin. The plugin looks into the
         message key or content or qos and decides who is the master.
         </td>
         </tr>
      </table>
      </p>

      <p>
         A message can specify its domain as a key attribute:
      </p>
      <pre>
      &lt;key oid='football.49.results' domain='football'/>
      </pre>

      <br />

      <p>
      <table border="1">
         <tr><td>3</td><td>The current status of a cluster node</td></tr>
         <tr><td>key</td><td><pre>
   &lt;key oid='__sys__cluster.node.state:heron'>
      &lt;__sys__cluster.node.state/>
   &lt;/key>
         </pre></td></tr>
         <tr><td>content</td><td><pre>
   &lt;clusternode id='heron'>
      &lt;state>
         &lt;cpu id='0' idle='40'/>    &lt;!-- currently 60% load on first CPU -->
         &lt;cpu id='1' idle='44'/>
         &lt;ram free='12000'/>        &lt;!-- xmlBlaster server instance has 12 MB free memory -->
         &lt;performance bogomips='1205.86' idleIndex='20'/>
      &lt;/state>
   &lt;/clusternode>
         </pre></td></tr>
      </table>
      We need to investigate how other clusters communicate their
      current load in a standardized way.
      </p>


      <br />

      <p>
      <table border="1">
         <tr><td> </td><td>Quality of Service (QoS) of a message traversing a cluster</td><td>Comments</td></tr>
         <tr><td>qos</td><td><pre>
   &lt;qos>
      &lt;sender>joe&lt;/sender>
      &lt;route>
         &lt;node id='bilbo' stratum='2' timestamp='34460239640'/>
         &lt;node id='frodo' stratum='1' timestamp='34460239661'/>
         &lt;node id='heron' stratum='0' timestamp='34460239590'/>
      &lt;/route>
   &lt;/qos>
         </pre></td>
         <td>A message published to <i>bilbo</i> found its way over <i>frodo</i> to the master <i>heron</i>.
         </td>
         </tr>
      </table>
      </p>
   </example>

   <configuration where="server">
      <p>
      These parameters allow to configure the cluster default behavior.
      </p>
      <table border="1">
         <tr>
            <td>Property</td>
            <td>Default</td>
            <td>Description</td>
            <td>Implemented</td>
         </tr>
         <tr>
            <td>cluster</td>
            <td>false</td>
            <td>Switch on/off cluster support</td>
            <td>yes</td>
         </tr>
         <tr>
            <td>cluster.node.id</td>
            <td>167.92.1.4:7607</td>
            <td>The world wide unique name of this xmlBlaster instance (= cluster node id), 
                if not specified defaults to the listen port of one of your activated
                protocol drivers.
                If you specify the name yourself, you should use a unique name like <i>heron.mycompany.com</i>
            </td>
            <td>yes</td>
         </tr>
         <tr>
            <td>cluster.domainMapper.type</td>
            <td>DomainToMaster</td>
            <td>Specifies which domain to master mapping plugin to use (see xmlBlaster.properties)</td>
            <td>yes</td>
         </tr>
         <tr>
            <td>cluster.domainMapper.version</td>
            <td>1.0</td>
            <td>The plugin version to use</td>
            <td>yes</td>
         </tr>
         <tr>
            <td>cluster.loadBalancer.type</td>
            <td>RoundRobin</td>
            <td>Specifies which load balance plugin to use (see xmlBlaster.properties)</td>
            <td>yes</td>
         </tr>
         <tr>
            <td>cluster.loadBalancer.version</td>
            <td>1.0</td>
            <td>The plugin version to use</td>
            <td>yes</td>
         </tr>
      </table>
   </configuration>

   <see type="API">org.xmlBlaster.engine.cluster.ClusterManager</see>
   <see type="REQ">util.property</see>

   <author>ruff@swand.lake.de</author>
   <author>Heinrich.Goetzger@exploding-systems.de</author>
   <author>Michele.Laghi@swisscom.com</author>
   <date>2000 04 10</date>
   <revision>$Revision: 1.18 $</revision>
   <testcase status="OPEN">
      <name>-</name>
      <comment>-</comment>
      <test tool="SUITE"></test>
   </testcase>
</requirement>

