<?xml version='1.0' encoding='ISO-8859-1' ?>
<!DOCTYPE requirement SYSTEM "requirement.dtd">

<requirement id='cluster' type='NEW' prio='HIGH' status="CLOSED">
   <topic>Multiple xmlBlaster instances can build a cluster following the master/slave paradigm</topic>
   <description>
      <p>
      We define a cluster as a configuration where more than one xmlBlaster
      server instance is running and those instances know of each other.
      The instances may run on the same host or distributed over the internet.
      </p>
      <p>
      All clustering abilities for xmlBlaster reduce to the simple master/slave approach.
      This cluster approach is easy to understand as we are not leaving the MoM paradigm
      to support clustering.
      </p>
      <p>
      An important part in clustering is the <i>discovery and lookup</i>.
      How to find other cluster nodes and access in depth
      node informations from them, and how to keep those informations up to date.
      This is addressed with the publish/subscribe idea as well.
      XmlBlaster nodes store their cluster informations in messages, so other nodes
      can subsrcribe to this data. If necessary one xmlBlaster is running as a 'naming service'
      holding the informations of all available xmlBlaster instances.
      </p>
      <p>
      In the following examples, we use the term <i>xmlBlaster server instance</i> and
      <i>xmlBlaster node</i> or just <i>node</i> interchangeable.
      </p>
      <center><p>
      <img src="cluster.gif" border="0" title="XmlBlaster cluster example" alt="Example for a typical xmlBlaster cluster" />
      </p></center>
      <p>
      In this example we have three xmlBlaster instances running, each of them has
      a unique cluster node ID, here the names <i>heron</i>, <i>golan</i> and <i>avalon</i>.
      </p>
      <p>
      Each of the nodes have an arbitrary number of clients attached. The clients
      can publish or subscribe to any message in the cluster, and may send PtP messages
      to any other client.
      </p>
      
      <hr />

      <h3>
      Clustering in our sense covers the following topics:
      </h3>
      <ol>
      
      
      <li><h4>Scalability:</h4><br />
      A master xmlBlaster server instance allows to have any number of slave xmlBlaster server instances
      for specific message domains. These slaves can have further slaves again.
      This allows distributing messages to an almost unlimited number of clients.
      Note that one xmlBlaster node can be a master for some messages and a slave for
      other message types simultaneously.
      <p>
      <img src="cluster.scalability.gif" border="0" title="XmlBlaster cluster example" alt="Example for a typical xmlBlaster cluster" />
      </p>
      <p>
      The example shows a tree like configuration of xmlBlaster nodes. In this way
      we can connect an almost unlimited number of clients. Every child leaf supplies
      a certain amount of slaves, which supplies other slaves which finally supply
      clients with message updates. The slaves are caching the messages and respond to
      request of their clients directly. The cache is always up to date as it is real time
      updated according to the publish/subscribe paradigm. 
      With every child level in the tree the latency increases
      for typically 5-40 milliseconds (intranet) for new published message updates.
      Note that the publisher does not need to be connected to the master node, the client in the
      down left edge of the picture is publishing as well.
      </p>
      <p>
      We introduce the term <i>stratum</i> as the distance of a node from the master. This is done
      in analogy to the network time protocol (NTP). A stratum of 0 is the master itself,
      1 is the first slave and stratum=2 would be bilbo in the above picture.
      </p>
      <p>
      Implementation status:
      </p>
      <table border="1">
         <tr>
            <th>Mode</th>
            <th>Description</th>
            <th>Hot</th>
            <th>Impl</th>
         </tr>

         <tr>
            <td>Publish/Subscribe</td>
            <td>
            This feature is implemented for Publish/Subscribe and ready for production use.<br />
            Changing the cluster configuration in hot operation is addressed
            by the design but final implementation and testing of this feature is missing.
            </td>
            <td><img src="no.gif" border="0" alt="no" /></td>
            <td><img src="ok.gif" border="0" alt="yes" /></td>
         </tr>
         <tr>
            <td>Point to point (PtP)</td>
            <td>
            PtP routing in cluster environment is ready available.
       If you destination address has an absolute name like '/node/heron/client/joe'
       the local node and all direct neighbors are checked and the message is
       directly delivered. Otherwise the same routing rule as for Publish/Subscribe apply.
            </td>
            <td><img src="no.gif" border="0" alt="no" /></td>
            <td><img src="ok.gif" border="0" alt="yes" /></td>
         </tr>
      </table>
      <br />
      </li>


      <li><h4>Availability (Failover):</h4><br />
      An xmlBlaster slave may adopt the master role for selective message types
      if the current xmlBlaster master fails.
      <p>
      <img src="cluster.availability.gif" border="0" title="XmlBlaster cluster example" alt="Example for a typical xmlBlaster cluster" />
      </p>
      <p>
      Autonomous failure recovery without distinct cluster manager (no single point of failure).
      </p>
      <p>
      We have three different failure situations to cover:
      </p>
      <ul>
      <li>Master failure:<br />
      This is the example in the above picture.
      </li>
      <li>Slave failure:<br />
      frodo dies, bilbo needs to rearrange (see figure 1).
      Bilbo needs to know the current cluster situation. Bilbo could choose to
      connect to a node with low load or with low stratum numbers for his
      messages.
      </li>
      <li>Client reconnect:<br />
      Client looses connection to its xmlBlaster instance and
      needs to find another one.
      </li>
      </ul>
      <p>
      Implementation status:
      </p>
      <table border="1">
         <tr>
            <th>Mode</th>
            <th>Description</th>
            <th>Hot</th>
            <th>Impl</th>
         </tr>

         <tr>
            <td>Publish/Subscribe</td>
            <td>-</td>
            <td><img src="no.gif" border="0" alt="no" /></td>
            <td><img src="no.gif" border="0" alt="no" /></td>
         </tr>
         <tr>
            <td>PtP</td>
            <td>-</td>
            <td><img src="no.gif" border="0" alt="no" /></td>
            <td><img src="no.gif" border="0" alt="no" /></td>
         </tr>
      </table>
      </li>


      <li><h4>Logical separation based on message domains:</h4><br />
      One xmlBlaster instance can be the master for selective messages (e.g. for
      stock exchange messages) and be slave for other messages (e.g. for 
      air traffic radar tracks or rugby news) simultaneously, we call that <i>cluster message domains</i>.
      Consequently a client interested in all informations only needs to connect
      to one xmlBlaster server instance.
      <p>
      <img src="cluster.domain.gif" border="0" title="XmlBlaster cluster example" alt="Example for a typical xmlBlaster cluster" />
      </p>
      <p>
      As we can see the node <i>heron</i> is master of messages of the domain "RUGBY_NEWS" but
      caches "STOCK_EXCHANGE" as well.
      </p>
      <p>
      Implementation status:
      </p>
      <table border="1">
         <tr>
            <th>Mode</th>
            <th>Description</th>
            <th>Hot</th>
            <th>Impl</th>
         </tr>

         <tr>
            <td>Publish/Subscribe</td>
            <td>
            This feature is implemented for Publish/Subscribe and ready for production use.
            Note that <code>erase()</code> calls to the slaves need to have the domain set in the XmlKey (similar to the publishes)
            to be forwarded to the master. <code>erase()</code> calls to the master are automatically propagated
            to all slaves, even with a missing domain setting.
            </td>
            <td><img src="no.gif" border="0" alt="no" /></td>
            <td><img src="ok.gif" border="0" alt="yes" /></td>
         </tr>
         <tr>
            <td>PtP</td>
            <td>
            This feature is implemented for PtP and ready for production use.   
            </td>
            <td><img src="no.gif" border="0" alt="no" /></td>
            <td><img src="ok.gif" border="0" alt="yes" /></td>
         </tr>
      </table>
      <br />
      </li>
      <li><h4>Load balancing:</h4><br />
      An xmlBlaster cluster allows to have more than one master server for
      a specific message domain. The master nodes are <i>mirrored</i> instances
      for those messages.
      Published messages reach all master nodes.
      Subscribed messages are retrieved using a load balance algorithm.
      <br />
      See <a href="http://www.ddj.com/documents/s=921/ddj9804i/9804i.htm" TARGET="others"> How to determine CPU load from Jaa with JNI</a>
      <p>
      <img src="cluster.loadbalance.gif" border="0" title="XmlBlaster cluster example" alt="Example for a typical xmlBlaster cluster" />
      </p>
      <p>
      In the above scenario heron1 and heron2 share their knowledge.
      Slave nodes can choose which of those servers to use.
      </p>
      <p>
      Implementation status:
      </p>
      <table border="1">
         <tr>
            <th>Mode</th>
            <th>Description</th>
            <th>Hot</th>
            <th>Impl</th>
         </tr>

         <tr>
            <td>Publish/Subscribe</td>
            <td>
            Mirroring of messges is possible in master/slave operation,
            mirroring of session stateful information is currently not implemented.
            </td>
            <td><img src="no.gif" border="0" alt="no" /></td>
            <td><img src="no.gif" border="0" alt="no" /></td>
         </tr>
         <tr>
            <td>PtP</td>
            <td>
            Mirroring of PtP messages is currently not supported as
            user session mirroring is not available.
            </td>
            <td><img src="no.gif" border="0" alt="no" /></td>
            <td><img src="no.gif" border="0" alt="no" /></td>
         </tr>
      </table>
      </li>
      </ol>

      <hr />


      <h3>
      Implementation overview:
      </h3>
      
      <p>
      We have to code and manage three logical mapping functionalities:
      </p>
      <ol>
         <li><h4>Find out who is the master of a message</h4>
         We have a message and need to map it to a master.
         The decision can be based on any information in the message.
         As a default we supply a simple key attribute based approach:
         An XML attribute <i>domain</i> is used with the
         message &lt;key> tag, for example:<br />
         <pre>&lt;key domain='STOCK_EXCHANGE'/></pre>
         The domain based approach maps domain names to cluster node IDs.
         Please see the examples below.<br />
         Note that this simple domain name approach has severe drawbacks:
         The clients need to know beforehand to which domain a message belongs. If the
         domains change, all clients need to be recompiled/restarted/reconfigured.
         A rule based approach (see the XPATH example in the Example section)
         is generic and addresses this issue.
         <p>
         The plugin interface <i>I_MapMsgToMasterId.java</i> allows you to code
         your own mapping logic, the default plugin delivered with xmlBlaster
         is <i>DomainToMaster.java</i> which implements a domain attribute based approach.
         </p>
         </li>
         <li><h4>Choose a node from the list of possible master nodes (load balancing)</h4>
         If step 1. has found more than one master nodes, a load balancing algorithm needs
         to choose one of those.
         <p>
         The plugin interface <i>I_LoadBalancer.java</i> allows you to code
         your own load balancing logic, the default plugin delivered with xmlBlaster
         is <i>RoundRobin.java</i> which implements a round robin approach.
         </p>
         </li>
         <li><h4>Map a cluster node ID to the physical xmlBlaster instance</h4>
         Now we need to find out how to reach the physical xmlBlaster instance.
         We have its cluster node ID and need to get the CORBA IOR, XmlRpc URL,
         RMI registry entry or the socket to be able to communicate.
         </li>
      </ol>
      
      <br />
      <p>
      The cluster specific features are
      </p>
            
      <ul>
      <li>Multi xmlBlaster instances<br />
      If multiple xmlBlaster servers are running, every message is assigned to exactly one
      master server (if no load balancing is switched on).
      The different xmlBlaster servers login to each other like other clients
      and act as slave to messages which they are not master from.
      </li>

      <li>Connections between xmlBlaster instances<br />
      This login is done in 'lazy' mode. As soon as a client requests a message
      for which the local server is not master, this server does a login to the master
      and subscribes the message from there. The message is than cached locally and further
      local requests from clients are handled locally. The cache is always up to date, as
      the slave has subscribed that info from the master.
      An adjustable expire date clears the cached message.
      </li>

      <li>Routing of published messages<br />
      If a message is published from a data source to a slave server, the message is routed directly
      to the master server (which may be a slave as well, forwarding the message to the real master
      with stratum level equals zero).
      If the master server is currently offline, it is queued
      for this 'client' until it logs in.<br />

      </li>

      <li>Plugin interface for master discovery<br />
      XmlBlaster supplies a plugin interface for your own logic to identify the master cluster node id.
      The default implementation uses the
      <pre>&lt;key domain='RUGBY'></pre> attribute (see examples below).
      </li>

      <li>Plugin interface for load balancing logic<br />
      XmlBlaster supplies a plugin interface for the load balancing logic.
      The default load balancing logic uses the round robin approach.<br />
      Further plugins may support Least Loaded Server(LLS),
      Threshold Loaded Sever(TLS), Least Memory Consumed Host (LMCH), see <a href="http://www.icmgworld.com/corp/k2/k2.features.asp" target="others">K2 Component server cluster features</a>.
      </li>

      <li>Default master<br />
      The xmlBlaster node is domain master for its directly connected clients
      as a default. If a client is publishing/subscribing etc. without an
      explicitly specified domain, the messages are handled in the local
      xmlBlaster instance (as if no cluster is existing).
      If acceptDefault='false' is set another node is searched which
      accepts default messages. A node can set acceptOtherDefault="true" to
      accept messages with default domain from other nodes.
      </li>

      <li>Master unknown<br />
      If a node receives a message and can't find the master,
      the message is accepted and queued for the master.
      If later a master appears the messages are flushed.
      </li>

      <li>Forward a client<br />
      If an xmlBlaster node is stopped, it has the ability to inform
      its clients and pass them a forward address of another
      xmlBlaster node which is willing to take care of them.
      </li>

      <li>PtP messages<br />
      To allow global delivery we introduce a unique naming schema,
      it is based on the URL naming synstax and allows addressing
      any client at any node.
      If you destination address has an absolute name like '/node/heron/client/joe'
      the local node and all direct neighbors are checked and if 'heron' is found the message is
      directly delivered. Otherwise the same routing rule as for Publish/Subscribe apply.
      For example a relative destination address 'client/joe/1' is routed similar
      to Publish/Subscribe by looking at the key-domain or other key attributes.
      If no routing matches the local node is chosen and the PtP message
      is queued for 'client/joe/1' until this client logs in.
      </li>

      <li>Multiple masters for same domain<br />
      If multiple nodes acquire the master mode for a certain domain
      there are two approaches.<br />
      Messages which are published are sent to all masters.<br />
      Messages which are accessed with get() or subscribe() are handled
      by the load balancing algorithm. The default implementation is a round robin.
      </li>

      <li>Behavior of the different xmlBlaster methods<br />
      XmlBlaster supports only a small number of methods to invoke.
      In the context of clustering they can be categorized into these groups:
        <ol>
         <li>Methods with local scope:<br />
           These are connect(), disconnect() and ping(). The have only
           a local scope between a client and its direct xmlBlaster node.
         </li>
         <li>Write access:<br />
           These are publish(), publishOneway() and erase(). Such invocations are passed
           to the master node. The new state than cascades the usual way
           to the connected slaves.
         </li>
         <li>Read access:<br />
           These are get() and subscribe() and unSubscribe().
           Such invocations are usually handled by the local xmlBlaster node,
           which itself forwards appropriate requests to the master.
         </li>
         <li>Callbacks:<br />
           These are update(), updateOneway() and ping().
           The behavior in a cluster is not specified yet.
         </li>
        </ol>
      </li>

      <li>Connection states between nodes<br />
      The connection of an xmlBlaster node to another is categorized into three states:
         <ol>
            <li>logged in: If the connection is up</li>
            <li>polling: If we have the address of the other node and are polling for it</li>
            <li>not allowed: We know a node but are not allowed to use it</li>
         </ol>
      </li>

      </ul>
   </description>

   <example lang="JAVA" type="HTML">
   <p>
     Please visit <i>xmlBlaster/demo/javaclients/cluster</i> for demos.
   </p>
   </example>

   <example lang="XML" type="HTML">
      Here are xmlBlaster internal messages which support clustering:
      <p>
      <table border="1">
         <tr><th>1</th><th>Mapping of a cluster node ID to a physical xmlBlaster instance</th><th>Comments</th></tr>
         <tr><td>key</td><td><pre>
&lt;key oid='__sys__cluster.node.info[heron]'>
   &lt;__sys__cluster.node.info/>
&lt;/key>
         </pre></td>
         <td>
         The connect tag contains a ConnectQos markup as described in 
         <a href="http://www.xmlBlaster.org/xmlBlaster/doc/requirements/interface.connect.html">the interface.connect requirement</a>
         </td>
         </tr>
         <tr><td>content</td><td><pre>
<!-- In future we could distinguish normal clients, slave clients
      and address based maxConnections -->

&lt;clusternode id='heron' maxConnections='800'>
   &lt;connect>
      &lt;qos>
         &lt;address type='IOR'>IOR:00044550005...&lt;/address>
         &lt;address type='XMLRPC' maxConnections='20'>
            http://www.mars.edu/RPC2
         &lt;/address>
         &lt;callback type='XMLRPC'>http://www.mars.universe:8081/RPC2&lt;/callback>
         &lt;backupnode>
            &lt;clusternode id='avalon'/>  &lt;!-- first failover node -->
            &lt;clusternode id='golan'/>   &lt;!-- second backup node -->
         &lt;/backupnode>
         &lt;nameservice>true&lt;/nameservice>
      &lt;/qos>
   &lt;connect>
   &lt;disconnect/>
&lt;/clusternode>
         </pre></td>
    <td>The backupnode setting is currently not implemented.
    The disconnect markup can be used to force a disconnect on cluster node shutdown,
    usually you won't set this to keep the connection alive in the remote server (to be able
    to collect messages during our shutdown).</td>
    </tr>
      </table>
      </p>

      <p>
      <table border="1">
         <tr><th>2</th><th>Determine the master: Mapping of messages to cluster node IDs<br />See <i>NodeDomainInfo.java</i> and plugin <i>DomainToMaster.java</i></th><th>Comments</th></tr>
         <tr><td>2a) key</td><td><pre>
&lt;key oid='__sys__cluster.node.master[heron]'>
   &lt;__sys__cluster.node.master/>
&lt;/key>
         </pre></td>
         <td>-</td>
         </tr>
         <tr><td>content</td><td><pre>
// This is a master for domainless messages and
// for football and rugby
&lt;clusternode id='heron'>
   &lt;master stratum='0' acceptOtherDefault='true'>
      &lt;key queryType='DOMAIN' domain='football'/>
      &lt;key queryType='DOMAIN' domain='rugby'/>
   &lt;/master>
&lt;/clusternode>
         </pre></td>
         <td>This cluster node is the master of the domain 'football' and 'rugby'.
         Messages without a domain specified are treated locally as well.</td>
         </tr>

         <tr><td>2b) key</td><td><pre>
&lt;key oid='__sys__cluster.node.master[frodo]'>
   &lt;__sys__cluster.node.master/>
&lt;/key>
         </pre></td>
         <td>-</td>
         </tr>
         <tr><td>content</td><td><pre>
// frodo is a slave for everything
&lt;clusternode id='frodo'>
   &lt;master stratum='0' acceptDefault='false'/>
                      &lt;!-- forward empty domains -->
   ...

// heron is master for everything (domain '*')
cluster.node[heron]=\
   &lt;clusternode id='heron'>\
      &lt;connect>&lt;qos>\
         &lt;address type='IOR' bootstrapHostname='' bootstrapPort='7600'/>\
      &lt;/qos>&lt;connect>\
      &lt;master type='DomainToMaster'>\
         &lt;key queryType='DOMAIN' domain='*'/>\
      &lt;/master>\
   &lt;/clusternode>
         </pre></td>
         <td>Messages without a domain specified are normally treated
         by its local xmlBlaster node. Here this is switched off.
    This cluster nodes is the master for all Pub/Sub messages because of the wildcard '*' setting</td>
         </tr>

         <tr><td>2c) key</td><td><pre>
&lt;key oid='__sys__cluster.node.master[bilbo]'>
   &lt;__sys__cluster.node.master/>
&lt;/key>
         </pre></td>
         <td>-</td>
         </tr>
         <tr><td>content</td><td><pre>
// Bilbo is master of RECIPIES and local clients,
// but slave for everything else
&lt;clusternode id='0'>
   &lt;master stratum='0'>
      &lt;key queryType='DOMAIN' domain=''/>
      &lt;key queryType='DOMAIN' domain='RECIPIES'/>
   &lt;/master>

   // refid points to a node one stratum closer to master
   &lt;master stratum='2' refid='frodo' />
   &lt;/master>
&lt;/clusternode>
         </pre></td>
         <td>Bilbo is slave of a slave for heron messages.
         Therefore it is stratum = 2 (two steps from the master).
         It only knows frodo, its direct parent node.
         </td>
         </tr>

         <tr><td>2d) key</td><td><pre>
&lt;key oid='__sys__cluster.node.master[heron]'>
   &lt;__sys__cluster.node.master/>
&lt;/key>
         </pre></td>
         <td>-</td>
         </tr>
         <tr><td>content</td><td><pre>
// The master is determined in a generic way
// (no explicit domain)
&lt;clusternode id='heron'>
   &lt;master>
      &lt;key queryType='EXACT' oid='radar.track'/>
      &lt;key queryType='XPATH'> //STOCK_EXCHANGE &lt;/key>
      &lt;filter type='ContentLength'>
              &lt;!-- Use your I_AccessFilter plugin -->
         8000 &lt;!-- Msg contents smaller 8000 bytes only -->
      &lt;/filter>
   &lt;/master>
&lt;/clusternode>
         </pre></td>
         <td>Approach without domains. Every message is filtered
         with the given rules. If one of the rules matches, we are the master
         of this message</td>
         </tr>

         <tr><td>2e) key</td><td><pre>
&lt;key oid='__sys__cluster.node.master[heron]'>
   &lt;__sys__cluster.node.master/>
&lt;/key>
         </pre></td>
         <td>-</td>
         </tr>
         <tr><td>content</td><td><pre>
// The master is determined with a customer plugin
// (no explicit domain)
&lt;clusternode id='heron'>
   &lt;master>
      Java plugin (implements I_MapMsgToMasterId)
   &lt;/master>
&lt;/clusternode>
         </pre></td>
         <td>Approach without domains. Every message is filtered by
         a user supplied plugin. The plugin looks into the
         message key or content or qos and decides who is the master.
         </td>
         </tr>
      </table>
      </p>

      <p>
         A message can specify its domain as a key attribute:
      </p>
      <pre>
      &lt;key oid='football.49.results' domain='football'/>
      </pre>

      <br />

      <p>
      <table border="1">
         <tr><th>3</th><th>The current status of a cluster node</th></tr>
         <tr><td>key</td><td><pre>
&lt;key oid='__sys__cluster.node.state[heron]'>
   &lt;__sys__cluster.node.state/>
&lt;/key>
         </pre></td></tr>
         <tr><td>content</td><td><pre>
&lt;clusternode id='heron'>
   &lt;state>
      &lt;cpu id='0' idle='40'/>   &lt;!-- currently 60% load on first CPU -->
      &lt;cpu id='1' idle='44'/>
      &lt;ram free='12000'/>       &lt;!-- xmlBlaster server has 12 MB free memory -->
      &lt;performance bogomips='1205.86' idleIndex='20'/>
   &lt;/state>
&lt;/clusternode>
         </pre></td></tr>
      </table>
      We need to investigate how other clusters communicate their
      current load in a standardized way.
      </p>


      <br />

      <p>
      <table border="1">
         <tr><th>-</th><th>Quality of Service (QoS) of a published message traversing a cluster</th><th>Comments</th></tr>
         <tr><td>qos</td><td><pre>
&lt;qos>
   &lt;sender>joe&lt;/sender>
   &lt;route>
      &lt;node id='bilbo' stratum='2' timestamp='34460239640'/>
      &lt;node id='frodo' stratum='1' timestamp='34460239661'/>
      &lt;node id='heron' stratum='0' timestamp='34460239590'/>
   &lt;/route>
&lt;/qos>
         </pre></td>
         <td>A message published to <i>bilbo</i> found its way over <i>frodo</i> to the master <i>heron</i>.
         </td>
         </tr>
      </table>
      </p>
      <p>
      This shows more complete the syntax of the configuration possibilities:
      </p>
      <pre>
&lt;clusternode id='heron.mycomp.com'>
   &lt;connect>&lt;qos>
      &lt;address type='IOR'>
         IOR:09456087000
      &lt;/address>
      &lt;address type='XMLRPC'>
         http://www.mycomp.com/XMLRPC/
      &lt;/address>
      &lt;callback type='RMI'>
         rmi://mycomp.com
      &lt;/callback>
   &lt;/qos>&lt;connect>
   
   &lt;master type='DomainToMaster' version='0.9'>
      &lt;![CDATA[
         &lt;key domain='RUGBY'/>
         &lt;key type='XPATH'>//STOCK&lt;/key>
      ]]&gt;
   &lt;/master>
   &lt;master stratum='1' refId='frodo' type='MyOwnMapperPlugin' version='2.0'>
      &lt;![CDATA[My own rule]]&gt;
   &lt;/master>

   &lt;state>
      &lt;cpu id='0' idle='40'/>
      &lt;cpu id='1' idle='44'/>
      &lt;ram free='12000'/>
   &lt;/state>
&lt;/clusternode>
      </pre>
      The return QoS value of a published message is if everything is OK as usual
      <pre>
&lt;qos>&lt;state id='OK'/>&lt;/qos>
      </pre>
      If the message can't be forwarded to the master node, it is tailed back
      by your local xmlBlaster node and flushed on reconnect to the master.
      The publish return QoS indicates the situation with a "FORWARD_WARNING"
      response:
      <pre>
&lt;qos>&lt;state id='FORWARD_WARNING'/>&lt;/qos>
      </pre>

   </example>

   <configuration where="server">
      <p>
      These parameters allow to configure the cluster behavior.
      </p>
      <p>The cluster manager is activated in the <code>xmlBlasterPlugins.xml</code> file,
      take care to have activated the protocol plugins you want to use
      for inter-cluster communication in an earlier run-level.
      </p>
<pre class="BORDER">
   &lt;plugin id='cluster' className='org.xmlBlaster.engine.cluster.ClusterManager'>
      &lt;action do='LOAD' onStartupRunlevel='5' sequence='5' />
      &lt;action do='STOP' onShutdownRunlevel='4' sequence='5'/>   
   &lt;/plugin>
</pre>
      <p>
      They can be set on command line, in the xmlBlaster.properties file or
      dynamically via messages.
      </p>
      <table border="1">
         <tr>
            <th>Property</th>
            <th>Default / Example</th>
            <th>Description</th>
            <th>Implemented</th>
         </tr>

         <tr>
            <td>cluster.node.id</td>
            <td> 167.92.1.4:7607<br />or<br /> heron.mycomp.com</td>
            <td>The world wide unique name of this xmlBlaster instance (= cluster node id), 
                if not specified defaults to the unique listen address of one of your activated
                protocol drivers.
                If you specify the name yourself, you should use a unique name like <i>heron.mycompany.com</i>
            </td>
            <td><img src="ok.gif" border="0" alt="yes" /></td>
         </tr>

         <tr>
            <td>cluster.loadBalancer.type</td>
            <td>RoundRobin</td>
            <td>Specifies which load balance plugin to use (see xmlBlaster.properties)</td>
            <td><img src="ok.gif" border="0" alt="yes" /></td>
         </tr>
         
         <tr>
            <td>cluster.loadBalancer.version</td>
            <td>1.0</td>
            <td>The plugin version to use</td>
            <td><img src="ok.gif" border="0" alt="yes" /></td>
         </tr>
         
         <tr>
            <td>cluster.node.info[heron]</td>
            <td><pre>
&lt;clusternode id='heron'>
   &lt;connect>&lt;qos>
      &lt;address type='SOCKET'>
         192.168.1.2:7607
      &lt;/address>
   &lt;/qos>&lt;connect>
&lt;/clusternode>
            </pre></td>
            <td>
            Configures how to access <i>heron</i>,
            replace the node name in the brackets with your specific xmlBlaster node.<br />
            NOTE: This setting can be overwritten by <i>__sys__cluster.node.info[heron]</i> messages.
            </td>
            <td><img src="ok.gif" border="0" alt="yes" /></td>
         </tr>

         <tr>
            <td>cluster.node.master[heron]</td>
            <td><pre>
&lt;clusternode id='heron'>
   &lt;master type='DomainToMaster'>
      &lt;![CDATA[
      &lt;key domain='RUGBY_NEWS'/>
      ]]&gt;
   &lt;/master>
&lt;/clusternode>
            </pre></td>
            <td>Configures for which message types <i>heron</i> is the master node.<br />
            NOTE: This setting can be overwritten by <i>__sys__cluster.node.master[heron]</i> messages.
            </td>
            <td><img src="ok.gif" border="0" alt="yes" /></td>
         </tr>

         <tr>
            <td>cluster.node[heron]</td>
            <td><pre>
&lt;clusternode id='heron'>
   &lt;connect>&lt;qos>
      &lt;address type='SOCKET'>
         192.168.1.2:7607
      &lt;/address>
   &lt;/qos>&lt;connect>
   &lt;master type='DomainToMaster'>
      &lt;![CDATA[
      &lt;key domain='RUGBY_NEWS'/>
      ]]&gt;
   &lt;/master>
&lt;/clusternode>
            </pre></td>
            <td>
            The combination of <i>cluster.node.info[...]</i> and <i>cluster.node.master[...]</i>
            to allow a more compact configuration.
            </td>
            <td><img src="ok.gif" border="0" alt="yes" /></td>
         </tr>

         <tr>
            <td>pingInterval[heron]<br />...</td>
            <td><pre>
-pingInterval 2000
-pingInterval[frodo] 1000
            </pre></td>
            <td>
            All client connection configuration settings are adjustable.<br />
            Try a<br />
            <code>java HelloWorld3 -help</code><br />
            for a list of current options.<br />
            Here we show as an example the ping interval, the time between the
            pings to another node in milliseconds.
            A given node <i>pingInterval[frodo]</i> has precedence over the <i>pingInterval</i> setting.
            This way you could tell xmlBlaster to ping its partner nodes every 2 seconds (pingInterval=2000)
            but to ping frodo more often (pingInterval=1000).
            </td>
            <td><img src="ok.gif" border="0" alt="yes" /></td>
         </tr>

         <tr>
            <td>passwd[bilbo]</td>
            <td>secret</td>
            <td>
            Allows to set the password for cluster node <code>bilbo</code>. Bilbo uses
            this password when it logs in to another xmlBlaster node.<br />
            You can't change the loginName of a cluster node. Every cluster node
            logs in to remote nodes with its cluser node id as the loginName.
            </td>
            <td><img src="ok.gif" border="0" alt="yes" /></td>
         </tr>
      </table>
   </configuration>

   <see type="REQ">cluster.dirtyRead</see>
   <see type="REQ">cluster.PtP</see>
   <see type="REQ">client.failsafe</see>
   <see type="REQ">util.property</see>
   <see type="REQ">interface.connect</see>
   <see type="API">org.xmlBlaster.engine.cluster.ClusterManager</see>
   <see type="API">org.xmlBlaster.engine.cluster.ClusterNode</see>
   <see type="API">org.xmlBlaster.engine.cluster.simpledomain.DomainToMaster</see>
   <see type="API">org.xmlBlaster.engine.cluster.simpledomain.RoundRobin</see>
   <see type="LOCAL">../../demo/javaclients/cluster/README</see>
   <see type="LOCAL">../../demo/javaclients/cluster/firewall/README</see>

   <author>xmlBlaster@marcelruff.info</author>
   <author>Heinrich.Goetzger@exploding-systems.de</author>
   <author>laghi@swissinfo.org</author>
   <date>2002 05 12</date>
   <revision>$Revision: 1.48 $</revision>
   <testcase status="CLOSED">
      <name>-</name>
      <comment>Tests a cluster setup as described in this requirement</comment>
      <test tool="SUITE">org.xmlBlaster.test.cluster.PublishTest</test>
      <test tool="SUITE">org.xmlBlaster.test.cluster.SubscribeTest</test>
      <test tool="SUITE">org.xmlBlaster.test.cluster.SubscribeXPathTest</test>
      <test tool="SUITE">org.xmlBlaster.test.cluster.EraseTest</test>
      <test tool="SUITE">org.xmlBlaster.test.cluster.DirtyReadTest</test>
      <test tool="SUITE">org.xmlBlaster.test.cluster.PtpTest</test>
   </testcase>
</requirement>

